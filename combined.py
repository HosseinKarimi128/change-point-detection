# Combined Python File
# This file is autogenerated by combine_python_files function

# ----- Begin _integ.py (Encoding: ascii) -----
import argparse
from pathlib import Path
import chardet # type: ignore
 

def detect_encoding(file_path, num_bytes=10000):
    """
    Detects the encoding of a file using chardet.

    Parameters:
    - file_path (Path): Path to the file.
    - num_bytes (int): Number of bytes to read for detection.

    Returns:
    - str: Detected encoding or 'utf-8' as default.
    """
    with file_path.open('rb') as f:
        rawdata = f.read(num_bytes)
    result = chardet.detect(rawdata)
    encoding = result['encoding']
    if not encoding:
        encoding = 'utf-8'
    return encoding

def combine_python_files(source_dir, output_file):
    """
    Combines all Python (.py) files in the specified directory and its immediate first-level subdirectories
    into a single Python file, excluding the output file itself.

    Parameters:
    - source_dir (str or Path): The directory containing Python files to combine.
    - output_file (str or Path): The path to the output Python file.

    Raises:
    - FileNotFoundError: If the source directory does not exist.
    - IOError: If there are issues reading or writing files.
    """
    source_path = Path(source_dir).resolve()
    output_path = Path(output_file).resolve()

    if not source_path.is_dir():
        raise FileNotFoundError(f"The directory {source_dir} does not exist.")

    # Initialize a set to store Python files
    python_files = set()

    # Add .py files from the top-level source directory
    top_level_files = sorted(source_path.glob("*.py"))
    python_files.update(top_level_files)

    # Add .py files from immediate first-level subdirectories
    first_level_dirs = [d for d in source_path.iterdir() if d.is_dir()]
    for sub_dir in first_level_dirs:
        sub_dir_py_files = sorted(sub_dir.glob("*.py"))
        python_files.update(sub_dir_py_files)

    # Convert the set to a sorted list
    python_files = sorted(python_files)

    # Exclude the output file itself if it's within the source directory or its first-level subdirectories
    python_files = [py_file for py_file in python_files if py_file.resolve() != output_path]

    if not python_files:
        print(f"No Python files found in directory and its immediate subdirectories: {source_dir}")
        return

    try:
        with output_path.open('w', encoding='utf-8') as outfile:
            outfile.write("# Combined Python File\n")
            outfile.write("# This file is autogenerated by combine_python_files function\n\n")
            
            for py_file in python_files:
                # Detect file encoding
                encoding = detect_encoding(py_file)
                
                # Compute the relative path for better readability in comments
                relative_path = py_file.relative_to(source_path)
                outfile.write(f"# ----- Begin {relative_path} (Encoding: {encoding}) -----\n")
                
                with py_file.open('r', encoding=encoding, errors='replace') as infile:
                    contents = infile.read()
                    outfile.write(contents)
                    outfile.write("\n\n")
                
                outfile.write(f"# ----- End {relative_path} -----\n\n")
        
        print(f"Successfully combined {len(python_files)} files into {output_file}")
    
    except IOError as e:
        print(f"An error occurred while reading or writing files: {e}")

def main():
    """
    Main function to parse command-line arguments and invoke the combine function.
    """
    parser = argparse.ArgumentParser(
        description="Combine all Python (.py) files in a directory and its immediate first-level subdirectories into a single Python file."
    )
    parser.add_argument(
        "source_dir",
        type=str,
        help="Path to the source directory containing Python files."
    )
    parser.add_argument(
        "output_file",
        nargs='?',
        default="combined.py",
        help="Path to the output Python file. Defaults to 'combined.py' in the current directory."
    )
    
    args = parser.parse_args()

    combine_python_files(args.source_dir, args.output_file)

if __name__ == "__main__":
    main()


# ----- End _integ.py -----

# ----- Begin main.py (Encoding: ascii) -----
# main.py

import argparse
import logging
from src.utils import setup_logging
from src.data_processing import (
    create_mega_df,
    add_delta_t_features,
    sample_and_scale,
    remove_nan_from_features,
    CustomDataset,
    CustomDatasetCNN
)
from src.model import trim_labels
from src.train import train_model
from src.predict import load_model, make_predictions, save_predictions_to_csv, plot_model_output
from sklearn.model_selection import train_test_split
import random
from pathlib import Path  # Import Path for handling file paths

def main(args):
    setup_logging()

    logger = logging.getLogger(__name__)
    logger.info("Project started")

    # Data paths for training
    if args.train:
        if not args.train_labels_paths or not args.train_features_paths:
            logger.error("Training requires --train_labels_paths and --train_features_paths.")
            return

        labels_paths = [Path(path) for path in args.train_labels_paths]
        features_paths = [Path(path) for path in args.train_features_paths]
    else:
        labels_paths = []
        features_paths = []

    # Data paths for prediction
    if args.predict:
        if not args.predict_labels_paths or not args.predict_features_paths:
            logger.error("Prediction requires --predict_labels_paths and --predict_features_paths.")
            return

        predict_labels_paths = [Path(path) for path in args.predict_labels_paths]
        predict_features_paths = [Path(path) for path in args.predict_features_paths]
    else:
        predict_labels_paths = []
        predict_features_paths = []

    # Common parameters
    max_len = 267  # Set to 267 to match the trained model

    if args.train:
        # Data Processing for Training
        mega_features, mega_labels = create_mega_df(labels_paths, features_paths, max_len)
        final_features = add_delta_t_features(mega_features)
        sampled_features, sampled_labels = sample_and_scale(final_features, mega_labels, sample_size=args.sample_size)

        # Label Trimming
        trimmed_labels = trim_labels(sampled_labels)

        # Train-validation split
        train_features, val_features, train_labels, val_labels = train_test_split(
            sampled_features, trimmed_labels, test_size=0.2, random_state=42
        )

        # Create datasets based on model type
        if args.model_type == 'lstm':
            train_dataset = CustomDataset(train_features, train_labels)
            val_dataset = CustomDataset(val_features, val_labels)
        elif args.model_type == 'cnn':
            train_dataset = CustomDatasetCNN(train_features, train_labels)
            val_dataset = CustomDatasetCNN(val_features, val_labels)
        else:
            raise ValueError("Unsupported model type. Choose 'lstm' or 'cnn'.")

        if args.train:
            # Training
            trainer = train_model(
                train_dataset,
                val_dataset,
                output_dir=args.output_dir,
                epochs=args.epochs,
                batch_size=args.batch_size,
                learning_rate=args.learning_rate,
                model_type=args.model_type
            )
            trainer.save_model(args.output_dir)
            logger.info(f"Model saved to {args.output_dir}")

    if args.predict:
        # Data Processing for Prediction
        mega_features, mega_labels = create_mega_df(predict_labels_paths, predict_features_paths, max_len)
        final_features = add_delta_t_features(mega_features)
        sampled_features, sampled_labels = sample_and_scale(final_features, mega_labels, sample_size=args.sample_size)

        # Label Trimming
        trimmed_labels = trim_labels(sampled_labels)

        # Load configuration based on model type
        from src.model import LSTMConfig, CNNConfig

        if args.model_type == 'lstm':
            config = LSTMConfig.from_pretrained(args.model_path)
        elif args.model_type == 'cnn':
            config = CNNConfig.from_pretrained(args.model_path)
        else:
            raise ValueError("Unsupported model type. Choose 'lstm' or 'cnn'.")

        # Load model
        model = load_model(args.model_path, config, model_type=args.model_type)

        # Prediction
        predictions = make_predictions(model, sampled_features, batch_size=args.batch_size)
        save_predictions_to_csv(predictions, trimmed_labels, save_path=args.predictions_csv)

        if args.save_plots:
            # Batch Sample Prediction: Generate multiple plots
            num_plot = args.num_plot_samples
            total_samples = len(sampled_features)

            if num_plot > total_samples:
                logger.warning(f"Requested number of plots ({num_plot}) exceeds the number of available samples ({total_samples}). Reducing to {total_samples}.")
                num_plot = total_samples

            # Select unique random sample indices
            sample_indices = random.sample(range(total_samples), k=num_plot)

            for idx, sample_idx in enumerate(sample_indices, start=1):
                # Define a unique save path for each plot
                base_save_path = Path(args.plot_save_path)
                save_dir = base_save_path.parent
                save_dir.mkdir(parents=True, exist_ok=True)  # Ensure the directory exists

                # Create a save path with sample index
                save_path = save_dir / f"{base_save_path.stem}_sample_{sample_idx}{base_save_path.suffix}"

                # Generate plot for the sample
                plot_model_output(
                    model=model,
                    features=sampled_features,
                    labels=trimmed_labels,
                    sample_index=sample_idx,
                    model_name=args.model_type.upper(),
                    save_path=str(save_path)
                )
                logger.info(f"Plot saved to {save_path}")

            logger.info("Prediction process completed.")

    logger.info("Project completed successfully")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="CPD Project with LSTM and CNN Models")

    # Training arguments
    parser.add_argument('--train', action='store_true', help='Flag to trigger training')
    parser.add_argument('--train_labels_paths', nargs='+', type=str, help='Paths to training labels CSV files')
    parser.add_argument('--train_features_paths', nargs='+', type=str, help='Paths to training features CSV files')
    parser.add_argument('--epochs', type=int, default=100, help='Number of training epochs')
    parser.add_argument('--batch_size', type=int, default=32, help='Batch size for training and prediction')
    parser.add_argument('--learning_rate', type=float, default=0.001, help='Learning rate for the optimizer')
    parser.add_argument('--output_dir', type=str, default='results', help='Directory to save the trained model')

    # Prediction arguments
    parser.add_argument('--predict', action='store_true', help='Flag to trigger prediction')
    parser.add_argument('--predict_labels_paths', nargs='+', type=str, help='Paths to prediction labels CSV files')
    parser.add_argument('--predict_features_paths', nargs='+', type=str, help='Paths to prediction features CSV files')
    parser.add_argument('--model_path', type=str, default='results/model', help='Path to the trained model')
    parser.add_argument('--predictions_csv', type=str, default='results/predictions.csv', help='Path to save the predictions CSV')
    parser.add_argument('--plot_save_path', type=str, default='results/model_output_sample.png', help='Base path to save the prediction plots')
    parser.add_argument('--save_plots', action='store_true', help='Flag to enable plot creation during prediction')
    parser.add_argument('--num_plot_samples', type=int, default=1, help='Number of sample plots to generate during prediction')

    # General arguments
    parser.add_argument('--sample_size', type=int, default=1000, help='Number of samples to process for prediction')
    parser.add_argument('--model_type', type=str, default='lstm', choices=['lstm', 'cnn'], help='Type of model to use: "lstm" or "cnn"')

    args = parser.parse_args()
    main(args)


# ----- End main.py -----

# ----- Begin src/__init__.py (Encoding: utf-8) -----


# ----- End src/__init__.py -----

# ----- Begin src/data_processing.py (Encoding: ascii) -----
# src/data_processing.py

import os
import pandas as pd
import numpy as np
import torch
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from torch.utils.data import Dataset
import logging
from tqdm import tqdm
tqdm.pandas()
logger = logging.getLogger(__name__)

def pad_data_frame(df, max_len):
    logger.debug(f"Padding DataFrame to max length {max_len}")
    return df.reindex(columns=range(max_len), fill_value=0)

def truncate_data_frame(df, max_len):
    logger.debug(f"Truncating DataFrame to max length {max_len}")
    return df.iloc[:, :max_len]

def create_mega_df(labels, features, max_len):
    logger.info("Creating mega DataFrame from labels and features")
    all_features = []
    all_labels = []

    for i in range(len(labels)):
        features_path = features[i]
        labels_path = labels[i]

        features_df = pd.read_csv(features_path, header=None)
        labels_df = pd.read_csv(labels_path, header=None)

        if len(features_df) < max_len:
            features_df = pad_data_frame(features_df, max_len)
        else:
            features_df = truncate_data_frame(features_df, max_len)

        if len(labels_df) < max_len:
            labels_df = pad_data_frame(labels_df, max_len)
        else:
            labels_df = truncate_data_frame(labels_df, max_len)

        all_features.append(features_df)
        all_labels.append(labels_df)
        logger.debug(f"Processed file {i+1}/{len(labels)}: Features shape {features_df.shape}")

    mega_features = pd.concat(all_features, axis=0).reset_index(drop=True)
    mega_labels = pd.concat(all_labels, axis=0).reset_index(drop=True)

    logger.info(f"Mega Features shape: {mega_features.shape}")
    logger.info(f"Mega Labels shape: {mega_labels.shape}")

    return mega_features, mega_labels

def create_delta_t_feature(sequence, max_len):
    sequence = np.nan_to_num(sequence)
    is_zero_mask = sequence == 0
    return count_ones_since_last_zero(is_zero_mask, max_len)

def count_ones_since_last_zero(arr, max_len):
    output = []
    count = 1
    for i in range(max_len):
        if arr[i]:
            output.append(count)
            count = 1
        else:
            output.append(0)
            count += 1
    return np.array(output)

def add_delta_t_features(mega_features):
    logger.info("Adding delta t features")
    max_len = mega_features.shape[1]
    delta_t = mega_features.progress_apply(lambda x: create_delta_t_feature(x, max_len), axis=1)
    final_features = np.dstack((mega_features.values, np.vstack(delta_t.values)))
    logger.info(f"Final features shape after adding delta t: {final_features.shape}")
    return final_features

def sample_and_scale(final_features, mega_labels, sample_size=1000):
    logger.info("Sampling and scaling features and labels")
    indices = np.random.choice(len(final_features), size=sample_size, replace=False)
    sampled_features = final_features[indices]
    sampled_labels = mega_labels.values[indices]

    # Separate features and delta_t features
    features = sampled_features[:, :, 0]
    delta_t = sampled_features[:, :, 1]

    # Scale features and delta_t separately
    scaler_features = StandardScaler()
    scaler_delta_t = StandardScaler()

    features = scaler_features.fit_transform(features)
    delta_t = scaler_delta_t.fit_transform(delta_t)

    # Combine scaled features
    scaled_features = np.stack((features, delta_t), axis=-1)

    sampled_features = torch.from_numpy(scaled_features).float()
    sampled_labels = torch.from_numpy(sampled_labels).float()

    logger.info(f"Sampled Features shape: {sampled_features.shape}")
    logger.info(f"Sampled Labels shape: {sampled_labels.shape}")

    return sampled_features, sampled_labels


def remove_nan_from_features(sampled_features, max_length=200):
    logger.info("Removing NaNs from features")
    non_nan_mask = ~torch.isnan(sampled_features[:, :, 0])
    output_features = []

    for i in range(sampled_features.size(0)):
        mask = non_nan_mask[i]
        cleaned = sampled_features[i][mask]
        if len(cleaned) > max_length:
            cleaned = cleaned[:max_length]
        else:
            padding = torch.zeros(max_length - len(cleaned), sampled_features.size(2))
            cleaned = torch.cat([cleaned, padding], dim=0)
        output_features.append(cleaned)

    output_features = torch.stack(output_features)
    logger.info(f"Output Features shape after NaN removal: {output_features.shape}")
    return output_features

# Custom Datasets
class CustomDataset(Dataset):
    def __init__(self, features, labels):
        self.features = features
        self.labels = labels

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return {'input_ids': self.features[idx], 'labels': self.labels[idx]}

class CustomDatasetCNN(Dataset):
    def __init__(self, features, labels):
        self.features = features
        self.labels = labels

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return {'input_ids': self.features[idx], 'labels': self.labels[idx]}


# ----- End src/data_processing.py -----

# ----- Begin src/model.py (Encoding: ascii) -----
# src/model.py

import torch
import torch.nn as nn
from transformers import PreTrainedModel, PretrainedConfig
import logging

logger = logging.getLogger(__name__)

def trim_labels(labels):
    logger.debug("Trimming labels to a maximum of 1.0")
    return torch.clamp(labels, max=1.0)

# Existing LSTM Models
class LSTMConfig(PretrainedConfig):
    model_type = "lstm"

    def __init__(self, hidden_size=64, num_layers=1, max_len=276, **kwargs):
        super().__init__(**kwargs)
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.max_len = max_len
        logger.debug(f"LSTMConfig initialized with hidden_size={hidden_size}, num_layers={num_layers}, max_len={max_len}")

class LSTMModel(PreTrainedModel):
    def __init__(self, config, verbose=False):
        super().__init__(config)
        self.model = _LSTM(input_size=2, hidden_size=config.hidden_size, num_layers=config.num_layers, max_len=config.max_len)
        self.sigmoid = nn.Sigmoid()
        if verbose:
            logger.info("LSTMModel initialized")

    def forward(self, input_ids, labels=None):
        x = self.model(input_ids)
        logits = self.sigmoid(x)
        loss = None
        if labels is not None:
            loss_fct = nn.BCELoss()
            loss = loss_fct(logits, labels)
        return {'loss': loss, 'logits': logits}

class _LSTM(nn.Module):
    def __init__(self, input_size=2, hidden_size=8, num_layers=1, max_len=276):
        super(_LSTM, self).__init__()
        self.lstm = nn.LSTM(input_size=input_size,
                            hidden_size=hidden_size,
                            num_layers=num_layers,
                            batch_first=True,
                            bidirectional=True)
        self.fc = nn.Linear(hidden_size * 2 * max_len, max_len)
        logger.debug(f"_LSTM initialized with hidden_size={hidden_size}, num_layers={num_layers}, max_len={max_len}")

    def forward(self, x):
        x, _ = self.lstm(x)
        x = x.reshape(x.size(0), -1)
        x = self.fc(x)
        return x

# Existing CNN Models
class CNNConfig(PretrainedConfig):
    model_type = "cnn"

    def __init__(self, seq_length=300, num_features=2, **kwargs):
        super().__init__(**kwargs)
        self.seq_length = seq_length
        self.num_features = num_features
        logger.debug(f"CNNConfig initialized with seq_length={seq_length}, num_features={num_features}")

class CNNModel(PreTrainedModel):
    def __init__(self, config, verbose=False):
        super().__init__(config)
        self.model = CNN1D(config.seq_length, config.num_features)
        self.sigmoid = nn.Sigmoid()
        if verbose:
            logger.info("CNNModel initialized")

    def forward(self, input_ids, labels=None):
        x = self.model(input_ids)
        logits = self.sigmoid(x)
        loss = None
        if labels is not None:
            loss_fct = nn.BCELoss()
            loss = loss_fct(logits, labels)
        return {'loss': loss, 'logits': logits}

class CNN1D(nn.Module):
    def __init__(self, seq_length, num_features=2):
        super(CNN1D, self).__init__()
        self.seq_length = seq_length
        self.num_features = num_features
        self.avg_pool = nn.AvgPool1d(kernel_size=30, stride=1, padding=14)
        self.conv = nn.Conv1d(in_channels=num_features, out_channels=32, kernel_size=(seq_length // 2) + 1)
        self.pool = nn.MaxPool1d(kernel_size=30, stride=1, padding=15)
        self.fc = nn.Linear(((seq_length // 2)) * 32, seq_length)
        self.relu = nn.ReLU()
        self.tanh = nn.Tanh()
        logger.debug(f"CNN1D initialized with seq_length={seq_length}, num_features={num_features}")

    def forward(self, x):
        x = x.permute(0, 2, 1)  # Change shape to (batch_size, channels, seq_length)
        x = self.avg_pool(x)
        x = self.tanh(self.conv(x))
        x = x.view(x.size(0), -1)  # Flatten
        x = self.fc(x)
        return x

# ----- New Attention LSTM Models -----
class AttentionLSTMConfig(PretrainedConfig):
    model_type = "attention_lstm"

    def __init__(self, hidden_size=64, num_layers=1, max_len=276, dropout=0.1, **kwargs):
        super().__init__(**kwargs)
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.max_len = max_len
        self.dropout = dropout
        logger.debug(f"AttentionLSTMConfig initialized with hidden_size={hidden_size}, num_layers={num_layers}, max_len={max_len}, dropout={dropout}")

class Attention(nn.Module):
    def __init__(self, hidden_size):
        super(Attention, self).__init__()
        self.attention = nn.Linear(hidden_size * 2, 1)  # *2 for bidirectional

    def forward(self, lstm_output):
        # lstm_output: [batch_size, seq_len, hidden_size*2]
        scores = self.attention(lstm_output)  # [batch_size, seq_len, 1]
        weights = torch.softmax(scores, dim=1)  # [batch_size, seq_len, 1]
        context = torch.sum(lstm_output * weights, dim=1)  # [batch_size, hidden_size*2]
        return context

class AttentionLSTMModel(PreTrainedModel):
    def __init__(self, config, verbose=False):
        super().__init__(config)
        self.model = _AttentionLSTM(input_size=2, hidden_size=config.hidden_size, num_layers=config.num_layers, max_len=config.max_len, dropout=config.dropout)
        self.sigmoid = nn.Sigmoid()
        if verbose:
            logger.info("AttentionLSTMModel initialized")

    def forward(self, input_ids, labels=None):
        x = self.model(input_ids)
        logits = self.sigmoid(x)
        loss = None
        if labels is not None:
            loss_fct = nn.BCELoss()
            loss = loss_fct(logits, labels)
        return {'loss': loss, 'logits': logits}

class _AttentionLSTM(nn.Module):
    def __init__(self, input_size=2, hidden_size=64, num_layers=1, max_len=276, dropout=0.1):
        super(_AttentionLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.max_len = max_len
        self.lstm = nn.LSTM(input_size=input_size,
                            hidden_size=hidden_size,
                            num_layers=num_layers,
                            batch_first=True,
                            bidirectional=True,
                            dropout=dropout if num_layers > 1 else 0)
        self.attention = Attention(hidden_size)
        self.fc = nn.Linear(hidden_size * 2, max_len)  # *2 for bidirectional
        logger.debug(f"_AttentionLSTM initialized with hidden_size={hidden_size}, num_layers={num_layers}, max_len={max_len}, dropout={dropout}")

    def forward(self, x):
        lstm_out, _ = self.lstm(x)  # lstm_out: [batch_size, seq_len, hidden_size*2]
        context = self.attention(lstm_out)  # context: [batch_size, hidden_size*2]
        logits = self.fc(context)  # logits: [batch_size, max_len]
        return logits

# ----- End src/model.py -----

# ----- Begin src/predict.py (Encoding: ascii) -----
# src/predict.py

import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend
import matplotlib.pyplot as plt
import torch
import pandas as pd
import random
from tqdm import tqdm
import logging
from .model import LSTMConfig, LSTMModel, CNNConfig, CNNModel, AttentionLSTMConfig, AttentionLSTMModel
from torch.utils.data import DataLoader
import os
import numpy as np

logger = logging.getLogger(__name__)

def load_model(model_path, config, model_type='lstm'):
    logger.info(f"Loading {model_type.upper()} model from {model_path}")

    if model_type == 'lstm':
        model = LSTMModel.from_pretrained(model_path, config=config)
    elif model_type == 'cnn':
        model = CNNModel.from_pretrained(model_path, config=config)
    elif model_type == 'attention_lstm':
        model = AttentionLSTMModel.from_pretrained(model_path, config=config)
    else:
        raise ValueError("Unsupported model type. Choose 'lstm', 'cnn', or 'attention_lstm'.")

    model.eval()
    logger.info(f"{model_type.upper()} model loaded successfully")
    return model

def make_predictions(model, features, batch_size=32):
    """
    Generates predictions using the trained model on the provided features.
    
    Args:
        model (PreTrainedModel): The trained model.
        features (torch.Tensor): The input features.
        batch_size (int, optional): Batch size for prediction. Defaults to 32.
    
    Returns:
        np.ndarray: Array of predictions with shape (num_samples, max_len).
    """
    logger.info("Starting prediction process")
    dataloader = DataLoader(features, batch_size=batch_size, shuffle=False)
    all_predictions = []

    for batch in tqdm(dataloader, desc="Predicting"):
        # Ensure batch is on the same device as the model
        batch = batch.to(next(model.parameters()).device)
        with torch.no_grad():
            outputs = model(batch)
        logits = outputs['logits']
        probabilities = logits.cpu().numpy()
        all_predictions.append(probabilities)
    
    predictions = np.vstack(all_predictions)
    logger.info(f"Predictions generated with shape: {predictions.shape}")
    return predictions

def save_predictions_to_csv(predictions, labels, save_path='predictions.csv'):
    """
    Saves the predictions and corresponding labels to a CSV file.
    
    Args:
        predictions (np.ndarray): Array of predictions with shape (num_samples, max_len).
        labels (torch.Tensor): Tensor of ground truth labels with shape (num_samples, max_len).
        save_path (str, optional): Path to save the CSV file. Defaults to 'predictions.csv'.
    """
    logger.info(f"Saving predictions to {save_path}")
    
    # Convert labels tensor to numpy array
    labels_np = labels.cpu().numpy()
    
    # Create a DataFrame with predictions and labels
    # Each row corresponds to a time step for a specific sample
    num_samples, max_len = predictions.shape
    data = {
        'Sample_Index': [],
        'Time_Step': [],
        'Prediction': [],
        'Ground_Truth': []
    }
    
    for sample_idx in range(num_samples):
        for time_step in range(max_len):
            data['Sample_Index'].append(sample_idx)
            data['Time_Step'].append(time_step)
            data['Prediction'].append(predictions[sample_idx, time_step])
            data['Ground_Truth'].append(labels_np[sample_idx, time_step])
    
    df = pd.DataFrame(data)
    df.to_csv(save_path, index=False)
    logger.info(f"Predictions successfully saved to {save_path}")

def plot_model_output(model, features, labels, sample_index=None, model_name="Model", save_path='model_output_sample.png'):
    """
    Plots the model output for a specific sample and saves it to a file.

    Args:
        model (PreTrainedModel): The trained model.
        features (torch.Tensor): The input features.
        labels (torch.Tensor): The ground truth labels.
        sample_index (int, optional): Index of the sample to plot. Defaults to a random sample.
        model_name (str, optional): Name of the model for the plot title. Defaults to "Model".
        save_path (str, optional): Path to save the plot image. Defaults to 'model_output_sample.png'.
    """
    if sample_index is None:
        sample_index = random.randint(0, len(features) - 1)
    logger.info(f"Plotting model output for sample index {sample_index}")

    input_tensor = features[sample_index].unsqueeze(0)
    with torch.no_grad():
        outputs = model(input_tensor)
    logits = outputs['logits'].squeeze().cpu().numpy()
    probabilities = torch.tensor(logits).numpy()
    ground_truth = labels[sample_index].cpu().numpy()

    fig, ax = plt.subplots(figsize=(25, 5))
    ax2 = ax.twinx()

    feature_plot = features[sample_index].cpu().numpy()
    
    ax.scatter(range(len(feature_plot)), feature_plot[:, 0], color='r', alpha=0.5, label='Displacements')
    ax.set_xlabel('Time Steps')
    ax.set_ylabel('Displacement Values', color='r')
    ax.tick_params(axis='y', labelcolor='r')

    ax2.plot(probabilities, color='b', label='Probabilities')
    for i,g in enumerate(ground_truth):
        if g == 1:
            ax2.axvline(i, color='g', linestyle='--', alpha=0.5)
    ax2.set_ylabel('Probability / Ground Truth', color='b')
    ax2.tick_params(axis='y', labelcolor='b')

    plt.title(f"{model_name}: Model Output")
    lines, labels_ = ax.get_legend_handles_labels()
    lines2, labels2 = ax2.get_legend_handles_labels()
    ax.legend(lines + lines2, labels_ + labels2, loc='upper left')
    plt.tight_layout()

    plt.savefig(save_path)
    logger.info(f"Plot saved to {save_path}")
    plt.close(fig)  # Close the figure to free memory


# ----- End src/predict.py -----

# ----- Begin src/train.py (Encoding: ascii) -----
# src/train.py

import logging
from transformers import Trainer, TrainingArguments
import torch
from .model import LSTMConfig, LSTMModel, CNNConfig, CNNModel, AttentionLSTMConfig, AttentionLSTMModel
from .data_processing import CustomDataset, CustomDatasetCNN

logger = logging.getLogger(__name__)

def train_model(train_dataset, val_dataset, output_dir="results", epochs=100, batch_size=32, learning_rate=0.001, model_type='lstm'):
    logger.info(f"Starting training process for {model_type.upper()} model")

    if model_type == 'lstm':
        config = LSTMConfig(hidden_size=64, num_layers=1, max_len=train_dataset.features.shape[1])
        model = LSTMModel(config=config, verbose=True)
    elif model_type == 'cnn':
        config = CNNConfig(seq_length=train_dataset.features.shape[1], num_features=train_dataset.features.shape[2])
        model = CNNModel(config=config, verbose=True)
    elif model_type == 'attention_lstm':
        config = AttentionLSTMConfig(hidden_size=64, num_layers=2, max_len=train_dataset.features.shape[1], dropout=0.3)
        model = AttentionLSTMModel(config=config, verbose=True)
    else:
        raise ValueError("Unsupported model type. Choose 'lstm', 'cnn', or 'attention_lstm'.")

    training_args = TrainingArguments(
        output_dir=output_dir,
        evaluation_strategy="epoch",
        learning_rate=learning_rate,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        num_train_epochs=epochs,
        save_steps=10000,
        save_total_limit=2,
        logging_dir='logs/',
        logging_steps=100,
        report_to=["tensorboard"]
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
    )

    trainer.train()
    logger.info("Training completed")

    return trainer

# ----- End src/train.py -----

# ----- Begin src/utils.py (Encoding: ascii) -----
# src/utils.py

import logging
import os

def setup_logging(log_file='logs/project.log'):
    """
    Sets up logging for the project.
    """
    os.makedirs(os.path.dirname(log_file), exist_ok=True)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )


# ----- End src/utils.py -----

